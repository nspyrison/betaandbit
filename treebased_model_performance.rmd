---
title: "Tree-based model perfromance"
subtitle: "On aggregated FIFA data (all quantitative)."
author: "Nicholas Spyrison"
date: "07 Mar 2022"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r opts_chunk, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
  echo    = TRUE,
  warning = FALSE, 
  message = FALSE
)
```

# Gist

We implement various __treebased models__, and look used to their higher-level performance metrics. We are working with the DALEX::fifa (2020 season) data. After aggregation we have 5000 observations of 8 _quantitative_ explanatory vars and will _regress_ wages [Euros]. I'll build a few models and evaluate performance with the unified API provided with __DALEX__.


# Setup

```{r}
require(DALEX)
require(magrittr)

## Remove a few alternative response variables
.dat_less_ys <- DALEX::fifa %>%
  dplyr::select(
    -c(`nationality`, `overall`, `potential`, `value_eur`, `wage_eur`)) %>%
  as.data.frame()

## Checking variable to aggregate
corrplot::corrplot(cor(.dat_less_ys),
                   method = "circle", ## geom
                   type   = "upper", ## only upper triangle
                   diag   = F, ## remove auto correlation
                   order  = "FPC", ## First principal component
                   tl.col = "black", tl.srt = 90, ## Text label color and rotation
                   tl.pos = "td")

## Aggregate some highly correlated vars
dat <- .dat_less_ys %>%
  dplyr::mutate(
    .keep = "none",
    BMI = (weight_kg+(height_cm/100L)^2L)/2L,
    age = age,
    react = movement_reactions,
    off = (attacking_finishing+skill_long_passing+attacking_volleys+
             power_long_shots+skill_curve+mentality_positioning+attacking_crossing+
             attacking_short_passing+skill_dribbling+skill_ball_control)/10L,
    def = (defending_sliding_tackle+mentality_interceptions+
             defending_standing_tackle+defending_marking+mentality_aggression)/5L,
    acc = (attacking_heading_accuracy+power_shot_power)/2L,
    mvm = (movement_sprint_speed+movement_balance+movement_acceleration+
             mentality_vision+mentality_composure+movement_agility+
             mentality_penalties+skill_fk_accuracy+power_stamina+movement_reactions)/10L,
    pwr = (power_strength+power_jumping)/2L,
    gk  = (goalkeeping_diving+goalkeeping_positioning+goalkeeping_reflexes+
             goalkeeping_handling+goalkeeping_kicking)/5L
  )
str(dat)
skimr::skim(dat)

## Let's get in the habit of train/test sets with something simple
X <- dat ## 9 aspects of the X's
Y <- DALEX::fifa$wage_eur 
set.seed(20220307)
idx_test <- sample(1:nrow(X), size = nrow(X) / 5)
X_train  <- X[-idx_test, ]
X_test   <- X[ idx_test, ]
Y_train  <- Y[-idx_test]
Y_test   <- Y[ idx_test]
remove(X, Y)
```


# Model creation

To start we'll set some modest hyperparameters and then fit several of the tree-based models that are compatible with __treeshap__ and __cheem__. While tree-based models all use decision trees, keep in mind that implementations and techniques differ especially when going to gradiant boosting and schochastic gradiant boosting to change the wight of the tree votes.

```{r}
## Set basic hyperparameters
n_tree <- nrow(X_train) %>% sqrt() %>% round() ## ~63
mtry   <- ncol(X_train) / 3 %>% round()        ## ~3
nodesz <- 5 # Ranger: 5 for regression
shrink <- .1

## Dependancies
require(randomForest)
require(ranger)
require(gmb)
require(xgboost)
require(tictoc)

## Model
tic("randomForest")
mod_rf <- randomForest::randomForest(
  x = X_train, y = Y_train,
  ntree = n_tree, mtry = mtry, nodesize = nodesz)
toc()
tic("ranger")
mod_rng <- ranger::ranger(
  x = X_train, y = Y_train,
  num.trees = n_tree, mtry = mtry, min.node.size = nodesz)
toc()
tic("gbm")
mod_gbm <- gbm::gbm( ## Has a wrapper for cfv, but will use just train set.
  Y ~ ., "gaussian", data.frame(Y = Y_train, X_train), 
  n.trees = n_tree, n.minobsinnode = nodesz, shrinkage = shrink)
toc()
tic("xgboost")
mod_xgb <- xgboost::xgboost(
  as.matrix(X_train), Y_train, 
  nrounds = sqrt(n_tree), eta = shrink, 
  params = list(objective = "reg:squarederror"), verbose = 0)
toc()
```

Great we have a battery of models to compare. Let's unify to a common interface and see if we can compare model performance.


# Model perfromance (train)

Keep in mind this is the highest level peak of the model explanation pyramid and evaluated on training data.

```{r}
if(interactive())
  ?DALEX::explain()
mod_ls <- list(mod_rf  = mod_rf,  mod_rng = mod_rng, 
               mod_gbm = mod_gbm, mod_xgb = mod_xgb)
nms    <- substr(names(mod_ls), 5, 99)

exp_ls <- mp_ls <- list()
measures_df <- data.frame(
  matrix(NA, nrow = length(nms), ncol = 4, 
         dimnames = list(nms, c("mse", "rmse", "r2", "mad")))
)

## For i in 1:length(models):
.mute <- sapply(seq_along(mod_ls), function(i){
  .X <- if(nms[i] == "xgb") as.matrix(X_train) else X_train
  ## DALEX method
  exp_ls[[i]] <<- DALEX::explain(
    model   = mod_ls[[i]],
    data    = .X,
    y       = Y_train,
    type    = "regression",
    label   = nms[i],
    verbose = FALSE
  )
  mp_ls[[i]] <<- DALEX::model_performance(exp_ls[[i]], cutoff = 0.05)
  measures_df[i, ] <<- unlist(mp_ls[[i]]$measures)
  
  ## Note that cheem:::model_performance_df errors on ranger model; app may need try catch
})
names(exp_ls) <- paste0("exp_", nms)
names(mp_ls)  <- paste0("mp_",  nms)

measures_df
plot(mp_ls)
plot(mp_ls, geom = "boxplot")
#plot(mp_ls, geom = "histogram")
```

Alright, randomForest and ranger are neck and neck. I'm quite surprised to see that gmb and xgb perform relatively poorly, half expected them to always outperform RF on non-tiny data, but this doesn't seem to be the case.


# Model perfromance -- test data

Keep in mind the above performance is evaluating on the train data, let's parallel these metrics for the random 20% of the data we save for testing the models.

```{r}
exp_ls_test <- mp_ls_test <- list()
measures_df_test <- data.frame(
  matrix(NA, nrow = length(nms), ncol = 4, 
         dimnames = list(nms, c("mse", "rmse", "r2", "mad")))
)
## For i in 1:length(models):
.mute <- sapply(seq_along(mod_ls), function(i){
  .X <- if(nms[i] == "xgb") as.matrix(X_test) else X_test
  exp_ls_test[[i]] <<- DALEX::explain(
    model   = mod_ls[[i]],
    data    = .X,
    y       = Y_test,
    type    = "regression",
    label   = nms[i],
    verbose = FALSE
  )
  mp_ls_test[[i]] <<- DALEX::model_performance(exp_ls_test[[i]], cutoff = 0.05)
  measures_df_test[i, ] <<- unlist(mp_ls_test[[i]]$measures)
})
names(exp_ls_test) <- paste0("exp_", nms)
names(mp_ls_test)  <- paste0("mp_",  nms)

measures_df_test
plot(mp_ls_test)
plot(mp_ls_test, geom = "boxplot")
```

Shocked to see high end outliers, likely a few star players with uniquely high salaries ended in the test set, but overall models are performing quite similarly now. I am also surprised to see that gbm and xgboost performed so well.


# Variable importance (train)

We want to know variable interpretations of these models as well. We should start my checking the Partial Dependence Plots of these models as well.


```{r}
vip_ls <- list()
## For i in 1:length(models):
.mute <- sapply(seq_along(mod_ls), function(i){
  vip_ls[[i]] <<- variable_importance(
    exp_ls[[i]], loss_function = loss_root_mean_square)
})
plot(vip_ls)
```

Position is concerning, but that aside it seems the random forests used the lower importance variables much more than the gbm, and xgb! I wonder if this is going to hold true in other cases due to the potentially much more complex interactions and number of terms.


# Bonus questions

## Does PCA improve performance -- test data

I suspect that modestly hyperparmetered data may see improved performance from PCA space rather than original variable because the components may lead to more informative split points. I would expect these gains to be lost as the hyperparameters are used more liberally. Let's briefly compare.

```{r}
## PCA space
pca <- prcomp(dat)
pca_train <- pca$x[-idx_test, ]
pca_test  <- pca$x[ idx_test, ]

## Create models
tic("randomForest")
mod_rf <- randomForest::randomForest(
  x = X_train, y = Y_train,
  ntree = n_tree, mtry = mtry, nodesize = nodesz)
toc()
tic("randomForest on PCA")
mod_rf_pca <- randomForest::randomForest(
  x = pca_train, y = Y_train,
  ntree = n_tree, mtry = mtry, nodesize = nodesz)
toc()

## Init
mod_ls <- list(mod_rf  = mod_rf,  mod_rf_pca = mod_rf_pca)
nms    <- substr(names(mod_ls), 5, 99)
exp_ls <- mp_ls <- list()
measures_df <- data.frame(
  matrix(NA, nrow = length(nms), ncol = 4, 
         dimnames = list(nms, c("mse", "rmse", "r2", "mad")))
)

## Compare model performance and VIP
exp_ls_pca <- mp_ls_pca <- vip_ls_pca <- list()
measures_df_pca <- data.frame(
  matrix(NA, nrow = length(nms), ncol = 4, 
         dimnames = list(nms, c("mse", "rmse", "r2", "mad")))
)
## For i in 1:length(models):
.mute <- sapply(seq_along(mod_ls), function(i){
  .X <- if(nms[i] == "rf_pca") pca_test else X_test
  exp_ls_pca[[i]] <<- DALEX::explain(
    model   = mod_ls[[i]],
    data    = .X,
    y       = Y_test,
    type    = "regression",
    label   = nms[i],
    verbose = FALSE
  )
  mp_ls_pca[[i]] <<- DALEX::model_performance(exp_ls_pca[[i]], cutoff = 0.05)
  measures_df_pca[i, ] <<- unlist(mp_ls_pca[[i]]$measures)
  vip_ls_pca[[i]] <<- variable_importance(
    exp_ls_pca[[i]], loss_function = loss_root_mean_square)
})
names(exp_ls_pca) <- paste0("exp_", nms)
names(mp_ls_pca)  <- paste0("mp_",  nms)
names(vip_ls_pca)  <- paste0("vip_",  nms)

measures_df_pca
plot(mp_ls_pca)
plot(mp_ls_pca, geom = "boxplot")
plot(vip_ls_pca)
```

Wow, we see a small loss in performance (on test data), when we first rotate the same data to PCA orientation. This was unexpected. The variable importance may be a little bit more front loaded, but with the added mapping back to the original orientation it's not worth it in this case.

To answer the question directly, not in this case, it lead to small decrease in performance. Do note that this is quantitative regression case.


# Session info

```{r}
## packages used
pkgs <- c(
  "DALEX",
  "randomForest",
  "ranger",
  "gmb",
  "xgboost"
)

## package & session info
devtools::session_info(pkgs)
```

# Related content

- https://ema.drwhy.ai/
- https://dalex.drwhy.ai/#examples
- https://github.com/MI2DataLab/ResponsibleML-UseR2021
- https://uc-r.github.io/dalex#procedures
