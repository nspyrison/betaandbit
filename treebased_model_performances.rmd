---
title: "Tree-based model perfromances"
  ## content also in  "Hitchhiker's Guide to Responsible machine learning"
subtitle: "Following FIFA as aggregated in cheem app."
author: "Nicholas Spyrison"
date: "07 Mar 2022"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r opts_chunk, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE)
```

# Gist

I am trying to get more familiar with different types of models get used to there higher-level performance metrics. We are working with the DALEX::fifa (2020 season) data. After aggregation we have 5000 observations of 8 quantitative explanatory vars and will regress wages [Euros]. I'll build a few models and evaluate performance with the unified API provided with __DALEX__


# Setup

```{r}
require(DALEX)
require(magrittr)

## Remove a few alternative response variables
.dat_less_ys <- DALEX::fifa %>%
  dplyr::select(
    -c(`nationality`, `overall`, `potential`, `value_eur`, `wage_eur`)) %>%
  as.data.frame()

## Checking variable to aggregate
corrplot::corrplot(cor(.dat_less_ys),
                   method = "circle", ## geom
                   type = "upper", ## only upper triangle
                   diag = F, ## remove auto correlation
                   order = "FPC", ## First principal component
                   tl.col = "black", tl.srt = 90, ## Text label color and rotation
                   tl.pos = "td")

## Aggregate some highly correlated vars
dat <- .dat_less_ys %>%
  dplyr::mutate(
    .keep = "none",
    BMI = (weight_kg+(height_cm/100L)^2L)/2L,
    age = age,
    react = movement_reactions,
    off = (attacking_finishing+skill_long_passing+attacking_volleys+
             power_long_shots+skill_curve+mentality_positioning+attacking_crossing+
             attacking_short_passing+skill_dribbling+skill_ball_control)/10L,
    def = (defending_sliding_tackle+mentality_interceptions+
             defending_standing_tackle+defending_marking+mentality_aggression)/5L,
    acc = (attacking_heading_accuracy+power_shot_power)/2L,
    mvm = (movement_sprint_speed+movement_balance+movement_acceleration+
             mentality_vision+mentality_composure+movement_agility+
             mentality_penalties+skill_fk_accuracy+power_stamina+movement_reactions)/10L,
    pwr = (power_strength+power_jumping)/2L,
    gk  = (goalkeeping_diving+goalkeeping_positioning+goalkeeping_reflexes+
             goalkeeping_handling+goalkeeping_kicking)/5L
  )

str(dat)
skimr::skim(dat)

X <- dat ## 9 aspects of the X's
Y <- DALEX::fifa$wage_eur 

## Let's get in the habit of train/test sets with something simple
set.seed(20220307)
idx_test <- sample(1:nrow(X), size = nrow(X)/5)
X_train <- X[-idx_test, ]
X_test  <- X[ idx_test, ]
Y_train <- Y[-idx_test]
Y_test  <- Y[ idx_test]
remove(X, Y)
```


# Model creation

I reckon we start with defaults and of tree-based method facilitated in cheem. We'll try to standardize hyperparameters to something modest, though keep in mind techniques are different.

```{r}
## Set basic hyperparameters
n_tree <- nrow(X_train) %>% sqrt() %>% round() ## ~63
mtry   <- ncol(X_train) / 3 %>% round()        ## ~3
nodesz <- 5 # Ranger: 5 for regression # nrow(X_train) / 500 %>% round()      ## ~8
shrink <- .1

## See basic form for treeshap supported models

require(randomForest)
require(ranger)
require(gmb)
require(xgboost)
require(tictoc)

## Model
tic("randomForest")
mod_rf <- randomForest::randomForest(
  x = X_train, y = Y_train, #xtest = X_test, ytest = Y_test,
  ntree = n_tree, mtry = mtry, nodesize = nodesz)
toc()
tic("ranger")
mod_rng <- ranger::ranger(
  x = X_train, y = Y_train, #alternatively #Y ~ ., data.frame(Y = Y_train, X_train),
  num.trees = n_tree, mtry = mtry, min.node.size = nodesz)
toc()
tic("gbm")
mod_gbm <- gbm::gbm( ## Has a wrapper for cfv, but will use just train set.
  Y ~ ., "gaussian", data.frame(Y = Y_train, X_train), 
  n.trees = n_tree, n.minobsinnode = nodesz, shrinkage = shrink)
toc()
tic("xgboost")
mod_xgb <- xgboost::xgboost(
  as.matrix(X_train), Y_train, 
  nrounds = sqrt(n_tree), eta = shrink, 
  params = list(objective = "reg:squarederror"), verbose = 0)
toc()
```

Great we have a battery of models to compare. Let's unify to a common interface and see if we can compare model performance


# Model perfromance (train)

Keep in mind this is the highest level peak of the model explanation pyramid and evaluated on training data.

```{r}
if(interactive())
  ?DALEX::explain()
mod_ls <- list(mod_rf  = mod_rf,  mod_rng = mod_rng, 
               mod_gbm = mod_gbm, mod_xgb = mod_xgb)
nms    <- substr(names(mod_ls), 5, 99)

exp_ls <- mp_ls <- list()
measures_df <- data.frame(
  matrix(NA, nrow = length(nms), ncol = 4, 
         dimnames = list(nms, c("mse", "rmse", "r2", "mad")))
)

## For i in 1:length(models):
.mute <- sapply(seq_along(mod_ls), function(i){
  .X <- if(nms[i] == "xgb") as.matrix(X_train) else X_train
  ## DALEX method
  exp_ls[[i]] <<- DALEX::explain(
    model            = mod_ls[[i]],
    #predict_function = predict(),
    data             = .X,
    y                = Y_train,
    type             = "regression",
    label            = nms[i],
    verbose          = FALSE
  )
  mp_ls[[i]] <<- DALEX::model_performance(exp_ls[[i]], cutoff = 0.05)
  measures_df[i, ] <<- unlist(mp_ls[[i]]$measures)
  
  # ## cheem unification and model performance
  # cmp_ls[[i]] <<- cheem:::model_performance_df(mod_ls[[i]], x = X_train)
  ## cheem:::model_performance_df errors on ranger model; 
  ## will need to change in package, or atleast try catch.
})
names(exp_ls) <- paste0("exp_", nms)
names(mp_ls)  <- paste0("mp_",  nms)

measures_df
plot(mp_ls)
plot(mp_ls, geom = "boxplot")
#plot(mp_ls, geom = "histogram")
```

Ok randomForest and ranger are neck and neck. I'm quite surprised to see that gmb and xgb perform relatively poorly, half expected them to always outperfrom RF on non-tiny data, but this doesn't seem to be the case.

# Model perfromance -- test data

Keep in mind the above performance is evaluating on the train data, let's parallel these metrics for the random 20% of the data we save for testing the models.

```{r}
exp_ls_test <- mp_ls_test <- list()
measures_df_test <- data.frame(
  matrix(NA, nrow = length(nms), ncol = 4, 
         dimnames = list(nms, c("mse", "rmse", "r2", "mad")))
)
## For i in 1:length(models):
.mute <- sapply(seq_along(mod_ls), function(i){
  .X <- if(nms[i] == "xgb") as.matrix(X_test) else X_test
  exp_ls_test[[i]] <<- DALEX::explain(
    model   = mod_ls[[i]],
    data    = .X,
    y       = Y_train,
    type    = "regression",
    label   = nms[i],
    verbose = FALSE
  )
  mp_ls_test[[i]] <<- DALEX::model_performance(exp_ls_test[[i]], cutoff = 0.05)
  measures_df_test[i, ] <<- unlist(mp_ls_test[[i]]$measures)
})
names(exp_ls_test) <- paste0("exp_", nms)
names(mp_ls_test)  <- paste0("mp_",  nms)

measures_df_test
plot(mp_ls_test)
plot(mp_ls_test, geom = "boxplot")
```

Shocked to see high end outliers, likely a few star players with uniquely high salaries ended in the test set, but overall models are performing quite similarly now. I am also surprised to see that gbm and xgboost performed so well.


# Variable importance (train)

We want to know variable interpretations of these models as well. We should start my checking the Partial Dependence Plots of these models as well.


```{r}
vip_ls <- list()
## For i in 1:length(models):
.mute <- sapply(seq_along(mod_ls), function(i){
  vip_ls[[i]] <<- variable_importance(
    exp_ls[[i]], loss_function = loss_root_mean_square)
})
plot(vip_ls)
```

Position is concerning, but that aside it seems the random forests used the lower importance variables much more than the gbm, and xgb! I wonder if this is going to hold true in other cases due to the potentially much more complex interactions and number of terms.




# Session info

```{r}
## packages used
pkgs <- c(
  "DALEX",
  "randomForest",
  "ranger",
  "gmb",
  "xgboost"
)

## package & session info
devtools::session_info(pkgs)
```

# Related content

- https://dalex.drwhy.ai/#examples
- https://github.com/MI2DataLab/ResponsibleML-UseR2021
- https://uc-r.github.io/dalex#procedures
- https://ema.drwhy.ai/